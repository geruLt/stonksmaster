{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e73bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ade5a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11453, 6, 60) (11453, 3)\n",
      "(2864, 6, 60) (2864, 3)\n",
      "----\n",
      "torch.Size([11453, 6, 60]) torch.Size([11453, 3])\n",
      "torch.Size([2864, 6, 60]) torch.Size([2864, 3])\n"
     ]
    }
   ],
   "source": [
    "# Get the data and split into train, validation and test sets\n",
    "X = np.load('data/X.npy')\n",
    "X = np.moveaxis(X, 1, 2)\n",
    "y = np.load('data/y.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test)) \n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(X_train_tensors, y_train_tensors),\n",
    "                              batch_size=64, shuffle=True, num_workers=8) \n",
    "test_dataloader = DataLoader(TensorDataset(X_test_tensors, y_test_tensors),\n",
    "                             batch_size=64, shuffle=True, num_workers=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f175a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_test(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_test, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8d9bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = X_train.shape[2] #number of features\n",
    "hidden_size = 64 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 3 \n",
    "\n",
    "model = LSTM_test(num_classes, input_size, hidden_size, num_layers) \n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f35139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_logger(history, data):\n",
    "    for idx, key in enumerate(history):\n",
    "        history[key].append(data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b640c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.02161, accuracy: 0.50371, val_loss: 1.01133, val_accuracy: 0.50803\n",
      "Epoch: 1, loss: 1.02004, accuracy: 0.50406, val_loss: 1.01002, val_accuracy: 0.50803\n",
      "Epoch: 2, loss: 1.01905, accuracy: 0.50406, val_loss: 1.01257, val_accuracy: 0.50803\n",
      "Epoch: 3, loss: 1.01836, accuracy: 0.50406, val_loss: 1.01345, val_accuracy: 0.50803\n",
      "Epoch: 4, loss: 1.01480, accuracy: 0.50397, val_loss: 0.99663, val_accuracy: 0.50803\n",
      "Epoch: 5, loss: 0.98966, accuracy: 0.51716, val_loss: 0.98157, val_accuracy: 0.53142\n",
      "Epoch: 6, loss: 0.97515, accuracy: 0.52667, val_loss: 0.96756, val_accuracy: 0.52793\n",
      "Epoch: 7, loss: 0.96881, accuracy: 0.53017, val_loss: 0.96502, val_accuracy: 0.53596\n",
      "Epoch: 8, loss: 0.96394, accuracy: 0.53174, val_loss: 0.96212, val_accuracy: 0.53736\n",
      "Epoch: 9, loss: 0.96381, accuracy: 0.53121, val_loss: 0.96498, val_accuracy: 0.53352\n",
      "Epoch: 10, loss: 0.96493, accuracy: 0.53217, val_loss: 0.97734, val_accuracy: 0.53492\n",
      "Epoch: 11, loss: 0.96278, accuracy: 0.53672, val_loss: 0.96858, val_accuracy: 0.53631\n",
      "Epoch: 12, loss: 0.96205, accuracy: 0.53436, val_loss: 0.95852, val_accuracy: 0.53841\n",
      "Epoch: 13, loss: 0.95987, accuracy: 0.53628, val_loss: 0.96086, val_accuracy: 0.53387\n",
      "Epoch: 14, loss: 0.95667, accuracy: 0.53811, val_loss: 0.95897, val_accuracy: 0.53876\n",
      "Epoch: 15, loss: 0.95545, accuracy: 0.53846, val_loss: 0.95705, val_accuracy: 0.54225\n",
      "Epoch: 16, loss: 0.94883, accuracy: 0.54003, val_loss: 0.96356, val_accuracy: 0.54434\n",
      "Epoch: 17, loss: 0.94443, accuracy: 0.54257, val_loss: 0.94482, val_accuracy: 0.54155\n",
      "Epoch: 18, loss: 0.94245, accuracy: 0.54126, val_loss: 0.95144, val_accuracy: 0.54120\n",
      "Epoch: 19, loss: 0.94000, accuracy: 0.54335, val_loss: 0.95388, val_accuracy: 0.53876\n",
      "Epoch: 20, loss: 0.93574, accuracy: 0.54518, val_loss: 0.96153, val_accuracy: 0.54120\n",
      "Epoch: 21, loss: 0.93417, accuracy: 0.55138, val_loss: 0.94964, val_accuracy: 0.53841\n",
      "Epoch: 22, loss: 0.93033, accuracy: 0.54571, val_loss: 0.94800, val_accuracy: 0.54015\n",
      "Epoch: 23, loss: 0.93059, accuracy: 0.55453, val_loss: 0.98084, val_accuracy: 0.50943\n",
      "Epoch: 24, loss: 0.93724, accuracy: 0.54990, val_loss: 0.94303, val_accuracy: 0.55098\n",
      "Epoch: 25, loss: 0.92964, accuracy: 0.55496, val_loss: 0.95594, val_accuracy: 0.54155\n",
      "Epoch: 26, loss: 0.93044, accuracy: 0.55435, val_loss: 0.95384, val_accuracy: 0.54365\n",
      "Epoch: 27, loss: 0.92853, accuracy: 0.55627, val_loss: 0.95313, val_accuracy: 0.53596\n",
      "Epoch: 28, loss: 0.92627, accuracy: 0.55531, val_loss: 0.94016, val_accuracy: 0.54155\n",
      "Epoch: 29, loss: 0.92489, accuracy: 0.55872, val_loss: 0.94548, val_accuracy: 0.53841\n",
      "Epoch: 30, loss: 0.92713, accuracy: 0.55269, val_loss: 0.95211, val_accuracy: 0.54190\n",
      "Epoch: 31, loss: 0.92760, accuracy: 0.55034, val_loss: 0.94391, val_accuracy: 0.54504\n",
      "Epoch: 32, loss: 0.92279, accuracy: 0.55191, val_loss: 0.94373, val_accuracy: 0.54295\n",
      "Epoch: 33, loss: 0.92229, accuracy: 0.55654, val_loss: 0.93985, val_accuracy: 0.53946\n",
      "Epoch: 34, loss: 0.92051, accuracy: 0.55715, val_loss: 0.94395, val_accuracy: 0.54015\n",
      "Epoch: 35, loss: 0.92284, accuracy: 0.55557, val_loss: 0.94336, val_accuracy: 0.53911\n",
      "Epoch: 36, loss: 0.91803, accuracy: 0.55601, val_loss: 0.93969, val_accuracy: 0.54155\n",
      "Epoch: 37, loss: 0.91783, accuracy: 0.55811, val_loss: 0.93446, val_accuracy: 0.54644\n",
      "Epoch: 38, loss: 0.91422, accuracy: 0.56064, val_loss: 0.93765, val_accuracy: 0.53666\n",
      "Epoch: 39, loss: 0.91596, accuracy: 0.56343, val_loss: 0.93760, val_accuracy: 0.54295\n",
      "Epoch: 40, loss: 0.90963, accuracy: 0.56448, val_loss: 0.94598, val_accuracy: 0.54155\n",
      "Epoch: 41, loss: 0.91204, accuracy: 0.55950, val_loss: 0.94382, val_accuracy: 0.54120\n",
      "Epoch: 42, loss: 0.90785, accuracy: 0.56439, val_loss: 0.94276, val_accuracy: 0.54190\n",
      "Epoch: 43, loss: 0.90661, accuracy: 0.56081, val_loss: 0.94322, val_accuracy: 0.54784\n",
      "Epoch: 44, loss: 0.90835, accuracy: 0.56335, val_loss: 0.95100, val_accuracy: 0.54504\n",
      "Epoch: 45, loss: 0.91087, accuracy: 0.56151, val_loss: 0.95509, val_accuracy: 0.53596\n",
      "Epoch: 46, loss: 0.91003, accuracy: 0.56151, val_loss: 0.95892, val_accuracy: 0.51606\n",
      "Epoch: 47, loss: 0.90697, accuracy: 0.56719, val_loss: 0.95581, val_accuracy: 0.53527\n",
      "Epoch: 48, loss: 0.90532, accuracy: 0.56308, val_loss: 0.96483, val_accuracy: 0.53911\n",
      "Epoch: 49, loss: 0.90136, accuracy: 0.56727, val_loss: 0.95284, val_accuracy: 0.54365\n",
      "Epoch: 50, loss: 0.90082, accuracy: 0.56562, val_loss: 0.95529, val_accuracy: 0.53911\n",
      "Epoch: 51, loss: 0.90446, accuracy: 0.56701, val_loss: 0.95472, val_accuracy: 0.54260\n",
      "Epoch: 52, loss: 0.90184, accuracy: 0.56727, val_loss: 0.97063, val_accuracy: 0.53108\n",
      "Epoch: 53, loss: 0.89888, accuracy: 0.56483, val_loss: 0.96608, val_accuracy: 0.53666\n",
      "Epoch: 54, loss: 0.89536, accuracy: 0.56946, val_loss: 0.95221, val_accuracy: 0.53980\n",
      "Epoch: 55, loss: 0.89523, accuracy: 0.56806, val_loss: 0.95933, val_accuracy: 0.54295\n",
      "Epoch: 56, loss: 0.89784, accuracy: 0.57112, val_loss: 0.95509, val_accuracy: 0.53980\n",
      "Epoch: 57, loss: 0.89654, accuracy: 0.56727, val_loss: 0.94657, val_accuracy: 0.53806\n",
      "Epoch: 58, loss: 0.89217, accuracy: 0.57312, val_loss: 0.96443, val_accuracy: 0.53631\n",
      "Epoch: 59, loss: 0.89171, accuracy: 0.56946, val_loss: 0.94899, val_accuracy: 0.53980\n",
      "Epoch: 60, loss: 0.88889, accuracy: 0.57391, val_loss: 0.96799, val_accuracy: 0.53771\n",
      "Epoch: 61, loss: 0.88945, accuracy: 0.57513, val_loss: 0.95179, val_accuracy: 0.55237\n",
      "Epoch: 62, loss: 0.89076, accuracy: 0.57478, val_loss: 0.94947, val_accuracy: 0.54504\n",
      "Epoch: 63, loss: 0.88593, accuracy: 0.57321, val_loss: 0.96003, val_accuracy: 0.54155\n",
      "Epoch: 64, loss: 0.88722, accuracy: 0.57339, val_loss: 0.95148, val_accuracy: 0.54050\n",
      "Epoch: 65, loss: 0.88855, accuracy: 0.57243, val_loss: 0.95576, val_accuracy: 0.53841\n",
      "Epoch: 66, loss: 0.88996, accuracy: 0.57225, val_loss: 0.96268, val_accuracy: 0.53876\n",
      "Epoch: 67, loss: 0.89422, accuracy: 0.57147, val_loss: 0.95122, val_accuracy: 0.54260\n",
      "Epoch: 68, loss: 0.89354, accuracy: 0.56876, val_loss: 0.97096, val_accuracy: 0.51676\n",
      "Epoch: 69, loss: 0.88815, accuracy: 0.57155, val_loss: 0.95441, val_accuracy: 0.54399\n",
      "Epoch: 70, loss: 0.88644, accuracy: 0.57330, val_loss: 0.96423, val_accuracy: 0.54225\n",
      "Epoch: 71, loss: 0.89243, accuracy: 0.56745, val_loss: 0.96377, val_accuracy: 0.53980\n",
      "Epoch: 72, loss: 0.89061, accuracy: 0.56955, val_loss: 0.95918, val_accuracy: 0.53876\n",
      "Epoch: 73, loss: 0.88185, accuracy: 0.57679, val_loss: 0.95057, val_accuracy: 0.54330\n",
      "Epoch: 74, loss: 0.87825, accuracy: 0.58046, val_loss: 0.94945, val_accuracy: 0.54434\n",
      "Epoch: 75, loss: 0.88283, accuracy: 0.57775, val_loss: 0.94843, val_accuracy: 0.53946\n",
      "Epoch: 76, loss: 0.87567, accuracy: 0.57697, val_loss: 0.94823, val_accuracy: 0.54330\n",
      "Epoch: 77, loss: 0.87568, accuracy: 0.57915, val_loss: 0.94988, val_accuracy: 0.54260\n",
      "Epoch: 78, loss: 0.87402, accuracy: 0.58159, val_loss: 0.96008, val_accuracy: 0.53457\n",
      "Epoch: 79, loss: 0.87084, accuracy: 0.58194, val_loss: 0.97432, val_accuracy: 0.52793\n",
      "Epoch: 80, loss: 0.87234, accuracy: 0.58247, val_loss: 0.96116, val_accuracy: 0.54853\n",
      "Epoch: 81, loss: 0.87335, accuracy: 0.58107, val_loss: 0.95441, val_accuracy: 0.54644\n",
      "Epoch: 82, loss: 0.87001, accuracy: 0.58971, val_loss: 0.95564, val_accuracy: 0.54085\n",
      "Epoch: 83, loss: 0.86649, accuracy: 0.59094, val_loss: 0.94601, val_accuracy: 0.55098\n",
      "Epoch: 84, loss: 0.86994, accuracy: 0.58491, val_loss: 0.94554, val_accuracy: 0.54469\n",
      "Epoch: 85, loss: 0.86910, accuracy: 0.58430, val_loss: 0.94228, val_accuracy: 0.55272\n",
      "Epoch: 86, loss: 0.86743, accuracy: 0.58963, val_loss: 0.94802, val_accuracy: 0.54958\n",
      "Epoch: 87, loss: 0.87133, accuracy: 0.58395, val_loss: 0.94183, val_accuracy: 0.54190\n",
      "Epoch: 88, loss: 0.87061, accuracy: 0.58421, val_loss: 0.94372, val_accuracy: 0.55482\n",
      "Epoch: 89, loss: 0.86628, accuracy: 0.58875, val_loss: 0.95430, val_accuracy: 0.54120\n",
      "Epoch: 90, loss: 0.86817, accuracy: 0.58710, val_loss: 0.94751, val_accuracy: 0.54539\n",
      "Epoch: 91, loss: 0.86498, accuracy: 0.59129, val_loss: 0.96316, val_accuracy: 0.53841\n",
      "Epoch: 92, loss: 0.86198, accuracy: 0.59033, val_loss: 0.97067, val_accuracy: 0.52200\n",
      "Epoch: 93, loss: 0.85865, accuracy: 0.59434, val_loss: 0.95959, val_accuracy: 0.54155\n",
      "Epoch: 94, loss: 0.85841, accuracy: 0.59618, val_loss: 0.95160, val_accuracy: 0.54818\n",
      "Epoch: 95, loss: 0.85622, accuracy: 0.59425, val_loss: 0.95116, val_accuracy: 0.54260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96, loss: 0.85403, accuracy: 0.59853, val_loss: 0.95563, val_accuracy: 0.56041\n",
      "Epoch: 97, loss: 0.85513, accuracy: 0.59443, val_loss: 0.94992, val_accuracy: 0.55866\n",
      "Epoch: 98, loss: 0.85411, accuracy: 0.59626, val_loss: 0.96567, val_accuracy: 0.55552\n",
      "Epoch: 99, loss: 0.85005, accuracy: 0.59932, val_loss: 0.94694, val_accuracy: 0.55901\n",
      "Epoch: 100, loss: 0.84606, accuracy: 0.60237, val_loss: 0.96695, val_accuracy: 0.55203\n",
      "Epoch: 101, loss: 0.85096, accuracy: 0.59670, val_loss: 0.96806, val_accuracy: 0.55342\n",
      "Epoch: 102, loss: 0.84773, accuracy: 0.60002, val_loss: 0.96151, val_accuracy: 0.55412\n",
      "Epoch: 103, loss: 0.84810, accuracy: 0.60220, val_loss: 0.95357, val_accuracy: 0.56634\n",
      "Epoch: 104, loss: 0.85403, accuracy: 0.59783, val_loss: 0.95600, val_accuracy: 0.55377\n",
      "Epoch: 105, loss: 0.85229, accuracy: 0.59923, val_loss: 0.95815, val_accuracy: 0.55517\n",
      "Epoch: 106, loss: 0.84758, accuracy: 0.59906, val_loss: 0.95325, val_accuracy: 0.55342\n",
      "Epoch: 107, loss: 0.84692, accuracy: 0.60010, val_loss: 0.96960, val_accuracy: 0.55622\n",
      "Epoch: 108, loss: 0.84522, accuracy: 0.60010, val_loss: 0.98975, val_accuracy: 0.54295\n",
      "Epoch: 109, loss: 0.83988, accuracy: 0.61023, val_loss: 0.95971, val_accuracy: 0.55587\n",
      "Epoch: 110, loss: 0.83747, accuracy: 0.60884, val_loss: 0.97625, val_accuracy: 0.53980\n",
      "Epoch: 111, loss: 0.83761, accuracy: 0.60927, val_loss: 0.97492, val_accuracy: 0.53736\n",
      "Epoch: 112, loss: 0.83270, accuracy: 0.60892, val_loss: 0.98603, val_accuracy: 0.55831\n",
      "Epoch: 113, loss: 0.83336, accuracy: 0.60971, val_loss: 0.96401, val_accuracy: 0.56075\n",
      "Epoch: 114, loss: 0.82572, accuracy: 0.61259, val_loss: 0.97157, val_accuracy: 0.55901\n",
      "Epoch: 115, loss: 0.83301, accuracy: 0.60919, val_loss: 0.96767, val_accuracy: 0.56669\n",
      "Epoch: 116, loss: 0.82928, accuracy: 0.61224, val_loss: 0.98176, val_accuracy: 0.54853\n",
      "Epoch: 117, loss: 0.83436, accuracy: 0.60692, val_loss: 0.97509, val_accuracy: 0.54888\n",
      "Epoch: 118, loss: 0.82498, accuracy: 0.61687, val_loss: 0.96828, val_accuracy: 0.55796\n",
      "Epoch: 119, loss: 0.82234, accuracy: 0.61329, val_loss: 0.99426, val_accuracy: 0.54539\n",
      "Epoch: 120, loss: 0.81957, accuracy: 0.61565, val_loss: 0.97611, val_accuracy: 0.56110\n",
      "Epoch: 121, loss: 0.81594, accuracy: 0.61399, val_loss: 0.99955, val_accuracy: 0.55971\n",
      "Epoch: 122, loss: 0.81694, accuracy: 0.61896, val_loss: 0.97791, val_accuracy: 0.56145\n",
      "Epoch: 123, loss: 0.81674, accuracy: 0.62115, val_loss: 0.97354, val_accuracy: 0.56459\n",
      "Epoch: 124, loss: 0.81835, accuracy: 0.62036, val_loss: 0.98232, val_accuracy: 0.56355\n",
      "Epoch: 125, loss: 0.82725, accuracy: 0.61233, val_loss: 0.99086, val_accuracy: 0.55237\n",
      "Epoch: 126, loss: 0.81654, accuracy: 0.62036, val_loss: 0.99339, val_accuracy: 0.54330\n",
      "Epoch: 127, loss: 0.81918, accuracy: 0.61600, val_loss: 0.98804, val_accuracy: 0.55761\n",
      "Epoch: 128, loss: 0.81719, accuracy: 0.62045, val_loss: 0.98430, val_accuracy: 0.54609\n",
      "Epoch: 129, loss: 0.81285, accuracy: 0.61984, val_loss: 0.99566, val_accuracy: 0.55342\n",
      "Epoch: 130, loss: 0.81379, accuracy: 0.61992, val_loss: 1.02314, val_accuracy: 0.54749\n",
      "Epoch: 131, loss: 0.80867, accuracy: 0.62272, val_loss: 1.00160, val_accuracy: 0.55307\n",
      "Epoch: 132, loss: 0.80963, accuracy: 0.61984, val_loss: 1.01564, val_accuracy: 0.54085\n",
      "Epoch: 133, loss: 0.80563, accuracy: 0.62455, val_loss: 0.98389, val_accuracy: 0.55517\n",
      "Epoch: 134, loss: 0.80189, accuracy: 0.62551, val_loss: 0.98506, val_accuracy: 0.55587\n",
      "Epoch: 135, loss: 0.79916, accuracy: 0.62761, val_loss: 0.99949, val_accuracy: 0.55656\n",
      "Epoch: 136, loss: 0.79912, accuracy: 0.63049, val_loss: 0.99626, val_accuracy: 0.55587\n",
      "Epoch: 137, loss: 0.79924, accuracy: 0.62962, val_loss: 0.99206, val_accuracy: 0.55237\n",
      "Epoch: 138, loss: 0.79323, accuracy: 0.63189, val_loss: 1.00864, val_accuracy: 0.54609\n",
      "Epoch: 139, loss: 0.78789, accuracy: 0.63520, val_loss: 1.00756, val_accuracy: 0.54679\n",
      "Epoch: 140, loss: 0.78778, accuracy: 0.63259, val_loss: 1.04465, val_accuracy: 0.54644\n",
      "Epoch: 141, loss: 0.80804, accuracy: 0.62132, val_loss: 1.01634, val_accuracy: 0.55133\n",
      "Epoch: 142, loss: 0.80171, accuracy: 0.62394, val_loss: 1.04159, val_accuracy: 0.53387\n",
      "Epoch: 143, loss: 0.80153, accuracy: 0.62717, val_loss: 1.01596, val_accuracy: 0.53492\n",
      "Epoch: 144, loss: 0.79903, accuracy: 0.62551, val_loss: 0.98764, val_accuracy: 0.55971\n",
      "Epoch: 145, loss: 0.79640, accuracy: 0.62805, val_loss: 1.02903, val_accuracy: 0.53527\n",
      "Epoch: 146, loss: 0.79912, accuracy: 0.62604, val_loss: 1.02180, val_accuracy: 0.54993\n",
      "Epoch: 147, loss: 0.78457, accuracy: 0.63101, val_loss: 1.00151, val_accuracy: 0.56390\n",
      "Epoch: 148, loss: 0.77923, accuracy: 0.64167, val_loss: 1.01270, val_accuracy: 0.55133\n",
      "Epoch: 149, loss: 0.77919, accuracy: 0.63809, val_loss: 0.99731, val_accuracy: 0.55831\n",
      "Epoch: 150, loss: 0.78041, accuracy: 0.64071, val_loss: 1.03978, val_accuracy: 0.53247\n",
      "Epoch: 151, loss: 0.77869, accuracy: 0.64254, val_loss: 1.00227, val_accuracy: 0.54993\n",
      "Epoch: 152, loss: 0.77397, accuracy: 0.64245, val_loss: 1.01009, val_accuracy: 0.55063\n",
      "Epoch: 153, loss: 0.77174, accuracy: 0.64088, val_loss: 1.01432, val_accuracy: 0.54365\n",
      "Epoch: 154, loss: 0.77611, accuracy: 0.63861, val_loss: 0.99859, val_accuracy: 0.55936\n",
      "Epoch: 155, loss: 0.77627, accuracy: 0.64254, val_loss: 1.01991, val_accuracy: 0.55133\n",
      "Epoch: 156, loss: 0.77780, accuracy: 0.64629, val_loss: 1.02628, val_accuracy: 0.55028\n",
      "Epoch: 157, loss: 0.78208, accuracy: 0.63730, val_loss: 1.01433, val_accuracy: 0.54365\n",
      "Epoch: 158, loss: 0.78884, accuracy: 0.63765, val_loss: 1.00980, val_accuracy: 0.55412\n",
      "Epoch: 159, loss: 0.78308, accuracy: 0.63311, val_loss: 0.99780, val_accuracy: 0.55936\n",
      "Epoch: 160, loss: 0.77695, accuracy: 0.64001, val_loss: 1.02415, val_accuracy: 0.54260\n",
      "Epoch: 161, loss: 0.78089, accuracy: 0.63756, val_loss: 1.02327, val_accuracy: 0.54050\n",
      "Epoch: 162, loss: 0.78202, accuracy: 0.63887, val_loss: 1.02792, val_accuracy: 0.54749\n",
      "Epoch: 163, loss: 0.78160, accuracy: 0.63913, val_loss: 1.03407, val_accuracy: 0.54958\n",
      "Epoch: 164, loss: 0.78026, accuracy: 0.63896, val_loss: 1.01675, val_accuracy: 0.55063\n",
      "Epoch: 165, loss: 0.77847, accuracy: 0.64385, val_loss: 1.01873, val_accuracy: 0.55412\n",
      "Epoch: 166, loss: 0.77936, accuracy: 0.64271, val_loss: 1.02117, val_accuracy: 0.55691\n",
      "Epoch: 167, loss: 0.77807, accuracy: 0.64219, val_loss: 1.03562, val_accuracy: 0.54015\n",
      "Epoch: 168, loss: 0.77776, accuracy: 0.64490, val_loss: 1.02796, val_accuracy: 0.54190\n",
      "Epoch: 169, loss: 0.78210, accuracy: 0.63948, val_loss: 1.01311, val_accuracy: 0.55691\n",
      "Epoch: 170, loss: 0.77660, accuracy: 0.64350, val_loss: 1.03503, val_accuracy: 0.55098\n",
      "Epoch: 171, loss: 0.77408, accuracy: 0.64586, val_loss: 1.01892, val_accuracy: 0.55377\n",
      "Epoch: 172, loss: 0.77354, accuracy: 0.64236, val_loss: 1.02329, val_accuracy: 0.54469\n",
      "Epoch: 173, loss: 0.76500, accuracy: 0.64787, val_loss: 1.05480, val_accuracy: 0.54784\n",
      "Epoch: 174, loss: 0.76652, accuracy: 0.64577, val_loss: 1.04996, val_accuracy: 0.54015\n",
      "Epoch: 175, loss: 0.76706, accuracy: 0.64656, val_loss: 1.03233, val_accuracy: 0.55866\n",
      "Epoch: 176, loss: 0.76179, accuracy: 0.65005, val_loss: 1.04102, val_accuracy: 0.55552\n",
      "Epoch: 177, loss: 0.76582, accuracy: 0.64533, val_loss: 1.07612, val_accuracy: 0.54679\n",
      "Epoch: 178, loss: 0.76485, accuracy: 0.64813, val_loss: 1.04662, val_accuracy: 0.53736\n",
      "Epoch: 179, loss: 0.75912, accuracy: 0.64935, val_loss: 1.04649, val_accuracy: 0.56320\n",
      "Epoch: 180, loss: 0.74849, accuracy: 0.65406, val_loss: 1.07453, val_accuracy: 0.55237\n",
      "Epoch: 181, loss: 0.76178, accuracy: 0.65101, val_loss: 1.04713, val_accuracy: 0.55761\n",
      "Epoch: 182, loss: 0.75401, accuracy: 0.65101, val_loss: 1.04565, val_accuracy: 0.56250\n",
      "Epoch: 183, loss: 0.74982, accuracy: 0.65852, val_loss: 1.06451, val_accuracy: 0.55831\n",
      "Epoch: 184, loss: 0.74916, accuracy: 0.65860, val_loss: 1.07473, val_accuracy: 0.55098\n",
      "Epoch: 185, loss: 0.75317, accuracy: 0.65450, val_loss: 1.07874, val_accuracy: 0.54679\n",
      "Epoch: 186, loss: 0.75548, accuracy: 0.65197, val_loss: 1.09474, val_accuracy: 0.53457\n",
      "Epoch: 187, loss: 0.75117, accuracy: 0.65782, val_loss: 1.09087, val_accuracy: 0.54749\n",
      "Epoch: 188, loss: 0.75085, accuracy: 0.65887, val_loss: 1.07376, val_accuracy: 0.55482\n",
      "Epoch: 189, loss: 0.74770, accuracy: 0.66157, val_loss: 1.08632, val_accuracy: 0.54469\n",
      "Epoch: 190, loss: 0.74852, accuracy: 0.66253, val_loss: 1.07519, val_accuracy: 0.55622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191, loss: 0.74805, accuracy: 0.65773, val_loss: 1.06995, val_accuracy: 0.53911\n",
      "Epoch: 192, loss: 0.73828, accuracy: 0.66463, val_loss: 1.08401, val_accuracy: 0.55482\n",
      "Epoch: 193, loss: 0.73985, accuracy: 0.66175, val_loss: 1.07050, val_accuracy: 0.53527\n",
      "Epoch: 194, loss: 0.73884, accuracy: 0.66384, val_loss: 1.06170, val_accuracy: 0.54295\n",
      "Epoch: 195, loss: 0.74125, accuracy: 0.66131, val_loss: 1.07071, val_accuracy: 0.53946\n",
      "Epoch: 196, loss: 0.73610, accuracy: 0.65974, val_loss: 1.10123, val_accuracy: 0.55063\n",
      "Epoch: 197, loss: 0.73401, accuracy: 0.66315, val_loss: 1.06825, val_accuracy: 0.55517\n",
      "Epoch: 198, loss: 0.72886, accuracy: 0.66603, val_loss: 1.07835, val_accuracy: 0.54539\n",
      "Epoch: 199, loss: 0.73928, accuracy: 0.65860, val_loss: 1.09201, val_accuracy: 0.53980\n",
      "Epoch: 200, loss: 0.73507, accuracy: 0.66009, val_loss: 1.10998, val_accuracy: 0.52025\n",
      "Epoch: 201, loss: 0.74051, accuracy: 0.66376, val_loss: 1.09837, val_accuracy: 0.53806\n",
      "Epoch: 202, loss: 0.73269, accuracy: 0.66480, val_loss: 1.10448, val_accuracy: 0.53911\n",
      "Epoch: 203, loss: 0.73435, accuracy: 0.66620, val_loss: 1.10421, val_accuracy: 0.54469\n",
      "Epoch: 204, loss: 0.72679, accuracy: 0.66594, val_loss: 1.09595, val_accuracy: 0.54644\n",
      "Epoch: 205, loss: 0.73433, accuracy: 0.66393, val_loss: 1.09819, val_accuracy: 0.53177\n",
      "Epoch: 206, loss: 0.74672, accuracy: 0.66315, val_loss: 1.10959, val_accuracy: 0.53806\n",
      "Epoch: 207, loss: 0.73314, accuracy: 0.66934, val_loss: 1.09928, val_accuracy: 0.54050\n",
      "Epoch: 208, loss: 0.73046, accuracy: 0.66515, val_loss: 1.07579, val_accuracy: 0.54958\n",
      "Epoch: 209, loss: 0.72660, accuracy: 0.67266, val_loss: 1.09729, val_accuracy: 0.54260\n",
      "Epoch: 210, loss: 0.72844, accuracy: 0.66585, val_loss: 1.07741, val_accuracy: 0.54190\n",
      "Epoch: 211, loss: 0.71736, accuracy: 0.67336, val_loss: 1.07080, val_accuracy: 0.55272\n",
      "Epoch: 212, loss: 0.72397, accuracy: 0.66926, val_loss: 1.09596, val_accuracy: 0.53946\n",
      "Epoch: 213, loss: 0.72616, accuracy: 0.66926, val_loss: 1.07619, val_accuracy: 0.55517\n",
      "Epoch: 214, loss: 0.73785, accuracy: 0.66498, val_loss: 1.10739, val_accuracy: 0.54295\n",
      "Epoch: 215, loss: 0.73720, accuracy: 0.66393, val_loss: 1.11139, val_accuracy: 0.55168\n",
      "Epoch: 216, loss: 0.73420, accuracy: 0.66830, val_loss: 1.08345, val_accuracy: 0.54504\n",
      "Epoch: 217, loss: 0.73130, accuracy: 0.66480, val_loss: 1.09981, val_accuracy: 0.54504\n",
      "Epoch: 218, loss: 0.73628, accuracy: 0.66218, val_loss: 1.09348, val_accuracy: 0.54818\n",
      "Epoch: 219, loss: 0.72381, accuracy: 0.66926, val_loss: 1.12568, val_accuracy: 0.53142\n",
      "Epoch: 220, loss: 0.72085, accuracy: 0.67048, val_loss: 1.09336, val_accuracy: 0.55237\n",
      "Epoch: 221, loss: 0.71776, accuracy: 0.67179, val_loss: 1.08132, val_accuracy: 0.54120\n",
      "Epoch: 222, loss: 0.71445, accuracy: 0.67668, val_loss: 1.10778, val_accuracy: 0.54644\n",
      "Epoch: 223, loss: 0.70755, accuracy: 0.67895, val_loss: 1.11164, val_accuracy: 0.55168\n",
      "Epoch: 224, loss: 0.70715, accuracy: 0.67642, val_loss: 1.13564, val_accuracy: 0.54085\n",
      "Epoch: 225, loss: 0.71473, accuracy: 0.67607, val_loss: 1.15390, val_accuracy: 0.54015\n",
      "Epoch: 226, loss: 0.70754, accuracy: 0.68070, val_loss: 1.11526, val_accuracy: 0.53876\n",
      "Epoch: 227, loss: 0.70507, accuracy: 0.67781, val_loss: 1.14218, val_accuracy: 0.53911\n",
      "Epoch: 228, loss: 0.71317, accuracy: 0.67650, val_loss: 1.13469, val_accuracy: 0.54399\n",
      "Epoch: 229, loss: 0.70641, accuracy: 0.67746, val_loss: 1.13612, val_accuracy: 0.54574\n",
      "Epoch: 230, loss: 0.69878, accuracy: 0.67982, val_loss: 1.15336, val_accuracy: 0.51816\n",
      "Epoch: 231, loss: 0.70441, accuracy: 0.68314, val_loss: 1.14586, val_accuracy: 0.54085\n",
      "Epoch: 232, loss: 0.70975, accuracy: 0.67982, val_loss: 1.16105, val_accuracy: 0.54050\n",
      "Epoch: 233, loss: 0.71325, accuracy: 0.67825, val_loss: 1.16529, val_accuracy: 0.53561\n",
      "Epoch: 234, loss: 0.72390, accuracy: 0.66934, val_loss: 1.15854, val_accuracy: 0.53073\n",
      "Epoch: 235, loss: 0.71865, accuracy: 0.66987, val_loss: 1.16436, val_accuracy: 0.53317\n",
      "Epoch: 236, loss: 0.72121, accuracy: 0.67030, val_loss: 1.12402, val_accuracy: 0.53701\n",
      "Epoch: 237, loss: 0.71630, accuracy: 0.67118, val_loss: 1.11598, val_accuracy: 0.53492\n",
      "Epoch: 238, loss: 0.71144, accuracy: 0.67502, val_loss: 1.14760, val_accuracy: 0.53946\n",
      "Epoch: 239, loss: 0.70408, accuracy: 0.68349, val_loss: 1.11920, val_accuracy: 0.54784\n",
      "Epoch: 240, loss: 0.70330, accuracy: 0.68070, val_loss: 1.13406, val_accuracy: 0.53911\n",
      "Epoch: 241, loss: 0.70927, accuracy: 0.67511, val_loss: 1.11847, val_accuracy: 0.54155\n",
      "Epoch: 242, loss: 0.70580, accuracy: 0.68017, val_loss: 1.11997, val_accuracy: 0.54749\n",
      "Epoch: 243, loss: 0.70136, accuracy: 0.68262, val_loss: 1.11279, val_accuracy: 0.53980\n",
      "Epoch: 244, loss: 0.69041, accuracy: 0.69187, val_loss: 1.12242, val_accuracy: 0.54853\n",
      "Epoch: 245, loss: 0.68734, accuracy: 0.68829, val_loss: 1.11778, val_accuracy: 0.55622\n",
      "Epoch: 246, loss: 0.69811, accuracy: 0.68838, val_loss: 1.13832, val_accuracy: 0.54155\n",
      "Epoch: 247, loss: 0.70154, accuracy: 0.68978, val_loss: 1.15131, val_accuracy: 0.54120\n",
      "Epoch: 248, loss: 0.70307, accuracy: 0.68419, val_loss: 1.13589, val_accuracy: 0.54574\n",
      "Epoch: 249, loss: 0.70386, accuracy: 0.68393, val_loss: 1.11527, val_accuracy: 0.54085\n",
      "Epoch: 250, loss: 0.70278, accuracy: 0.68279, val_loss: 1.11869, val_accuracy: 0.54714\n",
      "Epoch: 251, loss: 0.70624, accuracy: 0.68340, val_loss: 1.12886, val_accuracy: 0.55447\n",
      "Epoch: 252, loss: 0.69825, accuracy: 0.68812, val_loss: 1.12513, val_accuracy: 0.55028\n",
      "Epoch: 253, loss: 0.69051, accuracy: 0.68759, val_loss: 1.13699, val_accuracy: 0.54714\n",
      "Epoch: 254, loss: 0.68605, accuracy: 0.69274, val_loss: 1.13291, val_accuracy: 0.54714\n",
      "Epoch: 255, loss: 0.70123, accuracy: 0.68078, val_loss: 1.13875, val_accuracy: 0.55796\n",
      "Epoch: 256, loss: 0.68956, accuracy: 0.68663, val_loss: 1.14176, val_accuracy: 0.53771\n",
      "Epoch: 257, loss: 0.69143, accuracy: 0.69117, val_loss: 1.14807, val_accuracy: 0.54539\n",
      "Epoch: 258, loss: 0.68850, accuracy: 0.68960, val_loss: 1.15476, val_accuracy: 0.55028\n",
      "Epoch: 259, loss: 0.70254, accuracy: 0.68035, val_loss: 1.14261, val_accuracy: 0.55168\n",
      "Epoch: 260, loss: 0.68397, accuracy: 0.68916, val_loss: 1.14177, val_accuracy: 0.54609\n",
      "Epoch: 261, loss: 0.67621, accuracy: 0.69632, val_loss: 1.16032, val_accuracy: 0.54365\n",
      "Epoch: 262, loss: 0.67525, accuracy: 0.69868, val_loss: 1.18961, val_accuracy: 0.53631\n",
      "Epoch: 263, loss: 0.66496, accuracy: 0.70139, val_loss: 1.15764, val_accuracy: 0.54015\n",
      "Epoch: 264, loss: 0.66370, accuracy: 0.70191, val_loss: 1.14477, val_accuracy: 0.53073\n",
      "Epoch: 265, loss: 0.68052, accuracy: 0.69554, val_loss: 1.15680, val_accuracy: 0.53317\n",
      "Epoch: 266, loss: 0.68239, accuracy: 0.69423, val_loss: 1.14543, val_accuracy: 0.54050\n",
      "Epoch: 267, loss: 0.67613, accuracy: 0.69423, val_loss: 1.17480, val_accuracy: 0.52514\n",
      "Epoch: 268, loss: 0.67783, accuracy: 0.69589, val_loss: 1.14898, val_accuracy: 0.53527\n",
      "Epoch: 269, loss: 0.67416, accuracy: 0.69571, val_loss: 1.17652, val_accuracy: 0.52723\n",
      "Epoch: 270, loss: 0.67046, accuracy: 0.69580, val_loss: 1.13068, val_accuracy: 0.54679\n",
      "Epoch: 271, loss: 0.67355, accuracy: 0.69563, val_loss: 1.16836, val_accuracy: 0.54539\n",
      "Epoch: 272, loss: 0.67771, accuracy: 0.69790, val_loss: 1.15775, val_accuracy: 0.54015\n",
      "Epoch: 273, loss: 0.67820, accuracy: 0.69685, val_loss: 1.13886, val_accuracy: 0.53422\n",
      "Epoch: 274, loss: 0.66733, accuracy: 0.70052, val_loss: 1.16952, val_accuracy: 0.53457\n",
      "Epoch: 275, loss: 0.67817, accuracy: 0.69056, val_loss: 1.14401, val_accuracy: 0.53980\n",
      "Epoch: 276, loss: 0.67238, accuracy: 0.70086, val_loss: 1.18940, val_accuracy: 0.52828\n",
      "Epoch: 277, loss: 0.67969, accuracy: 0.69536, val_loss: 1.16502, val_accuracy: 0.54085\n",
      "Epoch: 278, loss: 0.67128, accuracy: 0.69798, val_loss: 1.15104, val_accuracy: 0.53666\n",
      "Epoch: 279, loss: 0.66713, accuracy: 0.70313, val_loss: 1.19116, val_accuracy: 0.52270\n",
      "Epoch: 280, loss: 0.66167, accuracy: 0.70156, val_loss: 1.15559, val_accuracy: 0.55098\n",
      "Epoch: 281, loss: 0.65201, accuracy: 0.70444, val_loss: 1.18360, val_accuracy: 0.55098\n",
      "Epoch: 282, loss: 0.65003, accuracy: 0.70706, val_loss: 1.18899, val_accuracy: 0.52654\n",
      "Epoch: 283, loss: 0.64933, accuracy: 0.71021, val_loss: 1.21151, val_accuracy: 0.52200\n",
      "Epoch: 284, loss: 0.65385, accuracy: 0.70916, val_loss: 1.19867, val_accuracy: 0.53177\n",
      "Epoch: 285, loss: 0.65127, accuracy: 0.70610, val_loss: 1.22801, val_accuracy: 0.52689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286, loss: 0.66068, accuracy: 0.70890, val_loss: 1.18571, val_accuracy: 0.53352\n",
      "Epoch: 287, loss: 0.65857, accuracy: 0.70855, val_loss: 1.22653, val_accuracy: 0.53561\n",
      "Epoch: 288, loss: 0.65958, accuracy: 0.70558, val_loss: 1.19295, val_accuracy: 0.53596\n",
      "Epoch: 289, loss: 0.64972, accuracy: 0.71352, val_loss: 1.21528, val_accuracy: 0.53596\n",
      "Epoch: 290, loss: 0.65371, accuracy: 0.71160, val_loss: 1.22689, val_accuracy: 0.53806\n",
      "Epoch: 291, loss: 0.65261, accuracy: 0.71422, val_loss: 1.22209, val_accuracy: 0.53736\n",
      "Epoch: 292, loss: 0.64832, accuracy: 0.71335, val_loss: 1.22842, val_accuracy: 0.53736\n",
      "Epoch: 293, loss: 0.65646, accuracy: 0.70637, val_loss: 1.25066, val_accuracy: 0.53422\n",
      "Epoch: 294, loss: 0.65829, accuracy: 0.70829, val_loss: 1.20272, val_accuracy: 0.53876\n",
      "Epoch: 295, loss: 0.67642, accuracy: 0.69571, val_loss: 1.22014, val_accuracy: 0.53980\n",
      "Epoch: 296, loss: 0.66926, accuracy: 0.69580, val_loss: 1.21092, val_accuracy: 0.53841\n",
      "Epoch: 297, loss: 0.67775, accuracy: 0.69964, val_loss: 1.20495, val_accuracy: 0.54399\n",
      "Epoch: 298, loss: 0.67841, accuracy: 0.69929, val_loss: 1.22729, val_accuracy: 0.55028\n",
      "Epoch: 299, loss: 0.68418, accuracy: 0.69475, val_loss: 1.21567, val_accuracy: 0.54539\n"
     ]
    }
   ],
   "source": [
    "history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0\n",
    "    val_loss_total = 0\n",
    "    correct_total = 0\n",
    "    val_correct_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        outputs = model.forward(samples[0]) #forward pass\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "        # obtain the loss function\n",
    "        loss = criterion(outputs, samples[1])\n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        correct_total += (samples[1].argmax(axis=1) == outputs.argmax(axis=1)).float().sum().item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx_test, samples in enumerate(test_dataloader):\n",
    "            outputs = model.forward(samples[0]) #forward pass\n",
    "            loss = criterion(outputs, samples[1])\n",
    "\n",
    "            val_loss_total += loss.item()\n",
    "            val_correct_total += (samples[1].argmax(axis=1) == outputs.argmax(axis=1)).float().sum().item() \n",
    "    \n",
    "    _loss = loss_total / (batch_idx+1)\n",
    "    _acc = correct_total / X_train.shape[0]\n",
    "    _val_loss = val_loss_total / (batch_idx_test+1)\n",
    "    _val_acc = val_correct_total / X_test.shape[0]\n",
    "    \n",
    "    history_logger(history, [_loss,  _val_loss, _acc, _val_acc])\n",
    "    \n",
    "    print(\"Epoch: %d, loss: %1.5f, accuracy: %1.5f, val_loss: %1.5f, val_accuracy: %1.5f\" % \\\n",
    "          (epoch, _loss, _acc, _val_loss, _val_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "318ae9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5037108181262552,\n",
       " 0.5040600715969615,\n",
       " 0.5040600715969615,\n",
       " 0.5040600715969615,\n",
       " 0.5039727582292849,\n",
       " 0.5171570767484502,\n",
       " 0.5266742338251986,\n",
       " 0.5301667685322623,\n",
       " 0.5317384091504409,\n",
       " 0.5312145289443814,\n",
       " 0.5321749759888239,\n",
       " 0.5367152711080067,\n",
       " 0.5343578101807387,\n",
       " 0.5362787042696237,\n",
       " 0.5381122849908321,\n",
       " 0.5384615384615384,\n",
       " 0.5400331790797172,\n",
       " 0.5425652667423383,\n",
       " 0.5412555662271894,\n",
       " 0.5433510870514275,\n",
       " 0.545184667772636,\n",
       " 0.551383916877674,\n",
       " 0.5457085479786955,\n",
       " 0.5545271981140313,\n",
       " 0.5498995896271719,\n",
       " 0.5549637649524142,\n",
       " 0.5543525713786781,\n",
       " 0.5562734654675631,\n",
       " 0.5553130184231205,\n",
       " 0.5587182397625077,\n",
       " 0.5526936173928229,\n",
       " 0.5503361564655549,\n",
       " 0.5519077970837335,\n",
       " 0.5565354055705929,\n",
       " 0.557146599144329,\n",
       " 0.5555749585261504,\n",
       " 0.5560115253645334,\n",
       " 0.5581070461887715,\n",
       " 0.5606391338513926,\n",
       " 0.5634331616170436,\n",
       " 0.5644809220291627,\n",
       " 0.5595040600715969,\n",
       " 0.564393608661486,\n",
       " 0.5608137605867458,\n",
       " 0.563345848249367,\n",
       " 0.5615122675281585,\n",
       " 0.5615122675281585,\n",
       " 0.567187636427137,\n",
       " 0.5630839081463372,\n",
       " 0.5672749497948136,\n",
       " 0.5656159958089584,\n",
       " 0.5670130096917838,\n",
       " 0.5672749497948136,\n",
       " 0.564830175499869,\n",
       " 0.5694577839867284,\n",
       " 0.5680607701039029,\n",
       " 0.5711167379725836,\n",
       " 0.5672749497948136,\n",
       " 0.5731249454291452,\n",
       " 0.5694577839867284,\n",
       " 0.5739107657382345,\n",
       " 0.5751331528857068,\n",
       " 0.5747838994150004,\n",
       " 0.5732122587968218,\n",
       " 0.573386885532175,\n",
       " 0.5724264384877324,\n",
       " 0.5722518117523793,\n",
       " 0.57146599144329,\n",
       " 0.5687592770453156,\n",
       " 0.5715533048109666,\n",
       " 0.5732995721644983,\n",
       " 0.5674495765301668,\n",
       " 0.569545097354405,\n",
       " 0.576792106871562,\n",
       " 0.5804592683139789,\n",
       " 0.5777525539160046,\n",
       " 0.5769667336069152,\n",
       " 0.57914956779883,\n",
       " 0.5815943420937746,\n",
       " 0.5819435955644809,\n",
       " 0.5824674757705405,\n",
       " 0.581070461887715,\n",
       " 0.5897144852876975,\n",
       " 0.5909368724351698,\n",
       " 0.584912250065485,\n",
       " 0.5843010564917489,\n",
       " 0.589627171920021,\n",
       " 0.5839518030210425,\n",
       " 0.5842137431240723,\n",
       " 0.588754038243255,\n",
       " 0.5870950842573998,\n",
       " 0.5912861259058761,\n",
       " 0.5903256788614337,\n",
       " 0.5943420937745569,\n",
       " 0.5961756744957653,\n",
       " 0.5942547804068803,\n",
       " 0.5985331354230332,\n",
       " 0.5944294071422335,\n",
       " 0.5962629878634419,\n",
       " 0.5993189557321226,\n",
       " 0.6023749236008032,\n",
       " 0.5966995547018249,\n",
       " 0.6000174626735353,\n",
       " 0.6022002968654501,\n",
       " 0.5978346284816205,\n",
       " 0.599231642364446,\n",
       " 0.5990570156290929,\n",
       " 0.6001047760412119,\n",
       " 0.6001047760412119,\n",
       " 0.6102331266916965,\n",
       " 0.6088361128088711,\n",
       " 0.609272679647254,\n",
       " 0.6089234261765476,\n",
       " 0.609709246485637,\n",
       " 0.6125905876189645,\n",
       " 0.6091853662795774,\n",
       " 0.6122413341482581,\n",
       " 0.606915218719986,\n",
       " 0.6168689426351175,\n",
       " 0.6132890945603772,\n",
       " 0.6156465554876451,\n",
       " 0.6139876015017899,\n",
       " 0.6189644634593556,\n",
       " 0.6211472976512704,\n",
       " 0.6203614773421811,\n",
       " 0.6123286475159347,\n",
       " 0.6203614773421811,\n",
       " 0.6159958089583515,\n",
       " 0.6204487907098577,\n",
       " 0.6198375971361215,\n",
       " 0.6199249105037982,\n",
       " 0.622718938269449,\n",
       " 0.6198375971361215,\n",
       " 0.6245525189906574,\n",
       " 0.6255129660351,\n",
       " 0.6276084868593381,\n",
       " 0.6304898279926657,\n",
       " 0.6296166943158997,\n",
       " 0.6318868418754912,\n",
       " 0.6352047498472017,\n",
       " 0.6325853488169039,\n",
       " 0.6213219243866236,\n",
       " 0.6239413254169214,\n",
       " 0.6271719200209552,\n",
       " 0.6255129660351,\n",
       " 0.6280450536977211,\n",
       " 0.6260368462411595,\n",
       " 0.6310137081987253,\n",
       " 0.6416659390552694,\n",
       " 0.6380860909805292,\n",
       " 0.6407054920108268,\n",
       " 0.6425390727320353,\n",
       " 0.6424517593643587,\n",
       " 0.64088011874618,\n",
       " 0.6386099711865887,\n",
       " 0.6425390727320353,\n",
       " 0.6462935475421288,\n",
       " 0.6373002706714398,\n",
       " 0.6376495241421462,\n",
       " 0.6331092290229634,\n",
       " 0.6400069850694141,\n",
       " 0.6375622107744696,\n",
       " 0.6388719112896184,\n",
       " 0.6391338513926482,\n",
       " 0.638959224657295,\n",
       " 0.6438487732471841,\n",
       " 0.6427136994673884,\n",
       " 0.6421898192613289,\n",
       " 0.6448965336593032,\n",
       " 0.6394831048633546,\n",
       " 0.6434995197764778,\n",
       " 0.6458569807037458,\n",
       " 0.6423644459966821,\n",
       " 0.6478651881603074,\n",
       " 0.6457696673360691,\n",
       " 0.6465554876451585,\n",
       " 0.6500480223522221,\n",
       " 0.6453331004976862,\n",
       " 0.6481271282633371,\n",
       " 0.6493495154108094,\n",
       " 0.6540644372653454,\n",
       " 0.6510084693966647,\n",
       " 0.6510084693966647,\n",
       " 0.6585174190168515,\n",
       " 0.6586047323845281,\n",
       " 0.6545010041037282,\n",
       " 0.6519689164411071,\n",
       " 0.6578189120754387,\n",
       " 0.6588666724875578,\n",
       " 0.6615733868855321,\n",
       " 0.6625338339299747,\n",
       " 0.6577315987077622,\n",
       " 0.6646293547542129,\n",
       " 0.6617480136208853,\n",
       " 0.6638435344451236,\n",
       " 0.6613114467825024,\n",
       " 0.6597398061643237,\n",
       " 0.6631450275037108,\n",
       " 0.6660263686370383,\n",
       " 0.6586047323845281,\n",
       " 0.6600890596350302,\n",
       " 0.6637562210774469,\n",
       " 0.6648039814895661,\n",
       " 0.6662009953723915,\n",
       " 0.6659390552693617,\n",
       " 0.6639308478128001,\n",
       " 0.6631450275037108,\n",
       " 0.6693442766087488,\n",
       " 0.6651532349602725,\n",
       " 0.6726621845804592,\n",
       " 0.6658517419016852,\n",
       " 0.673360691521872,\n",
       " 0.6692569632410722,\n",
       " 0.6692569632410722,\n",
       " 0.6649786082249193,\n",
       " 0.6639308478128001,\n",
       " 0.6682965161966297,\n",
       " 0.6648039814895661,\n",
       " 0.6621845804592683,\n",
       " 0.6692569632410722,\n",
       " 0.6704793503885444,\n",
       " 0.6717890509036933,\n",
       " 0.6766785994935824,\n",
       " 0.6789487470531739,\n",
       " 0.6764166593905527,\n",
       " 0.6760674059198464,\n",
       " 0.6806950144067057,\n",
       " 0.6778136732733782,\n",
       " 0.6765039727582293,\n",
       " 0.6774644198026718,\n",
       " 0.6798218807299398,\n",
       " 0.6831397887016503,\n",
       " 0.6798218807299398,\n",
       " 0.6782502401117612,\n",
       " 0.6693442766087488,\n",
       " 0.6698681568148084,\n",
       " 0.6703047236531913,\n",
       " 0.6711778573299572,\n",
       " 0.6750196455077272,\n",
       " 0.6834890421723566,\n",
       " 0.6806950144067057,\n",
       " 0.6751069588754038,\n",
       " 0.6801711342006461,\n",
       " 0.6826159084955907,\n",
       " 0.6918711254693094,\n",
       " 0.6882912773945691,\n",
       " 0.6883785907622457,\n",
       " 0.6897756046450711,\n",
       " 0.6841875491137693,\n",
       " 0.6839256090107395,\n",
       " 0.6827905352309439,\n",
       " 0.68340172880468,\n",
       " 0.6881166506592159,\n",
       " 0.6875927704531564,\n",
       " 0.6927442591460753,\n",
       " 0.6807823277743823,\n",
       " 0.6866323234087138,\n",
       " 0.6911726185278966,\n",
       " 0.689600977909718,\n",
       " 0.6803457609359993,\n",
       " 0.689164411071335,\n",
       " 0.6963241072208155,\n",
       " 0.6986815681480835,\n",
       " 0.7013882825460578,\n",
       " 0.7019121627521173,\n",
       " 0.6955382869117261,\n",
       " 0.6942285863965774,\n",
       " 0.6942285863965774,\n",
       " 0.6958875403824325,\n",
       " 0.6957129136470793,\n",
       " 0.695800227014756,\n",
       " 0.6956256002794028,\n",
       " 0.6978957478389941,\n",
       " 0.696847987426875,\n",
       " 0.7005151488692919,\n",
       " 0.6905614249541605,\n",
       " 0.7008644023399982,\n",
       " 0.695363660176373,\n",
       " 0.6979830612066708,\n",
       " 0.7031345498995897,\n",
       " 0.701562909281411,\n",
       " 0.7044442504147385,\n",
       " 0.7070636514450362,\n",
       " 0.7102069326813936,\n",
       " 0.7091591722692744,\n",
       " 0.7061032044005937,\n",
       " 0.7088972321662447,\n",
       " 0.7085479786955383,\n",
       " 0.7055793241945342,\n",
       " 0.713524840653104,\n",
       " 0.711603946564219,\n",
       " 0.7142233475945167,\n",
       " 0.7133502139177508,\n",
       " 0.7063651445036235,\n",
       " 0.7082860385925085,\n",
       " 0.6957129136470793,\n",
       " 0.695800227014756,\n",
       " 0.699642015192526,\n",
       " 0.6992927617218196,\n",
       " 0.6947524666026369]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "07c96f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oorakci\\AppData\\Local\\Temp/ipykernel_14628/3097220172.py:7: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(8,2))\n",
    "ax[0].plot(history['loss'], c='blue')\n",
    "ax[0].plot(history['acc'], c='orange')\n",
    "ax[1].plot(history['val_loss'], c='blue')\n",
    "ax[1].plot(history['val_acc'], c='orange')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3acff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
