{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e73bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ade5a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11453, 6, 60) (11453, 3)\n",
      "(2864, 6, 60) (2864, 3)\n",
      "----\n",
      "torch.Size([11453, 6, 60]) torch.Size([11453, 3])\n",
      "torch.Size([2864, 6, 60]) torch.Size([2864, 3])\n"
     ]
    }
   ],
   "source": [
    "# Get the data and split into train, validation and test sets\n",
    "X = np.load('data/X.npy')\n",
    "X = np.moveaxis(X, 1, 2)\n",
    "y = np.load('data/y.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test)) \n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(X_train_tensors, y_train_tensors),\n",
    "                              batch_size=64, shuffle=True, num_workers=8) \n",
    "test_dataloader = DataLoader(TensorDataset(X_test_tensors, y_test_tensors),\n",
    "                             batch_size=64, shuffle=True, num_workers=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f175a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_test(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_test, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8d9bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = X_train.shape[2] #number of features\n",
    "hidden_size = 64 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 3 \n",
    "\n",
    "model = LSTM_test(num_classes, input_size, hidden_size, num_layers) \n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "73a029de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_logger(history, data):\n",
    "    for idx, key in enumerate(history):\n",
    "        history[key].append(data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c486a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.02161, accuracy: 0.50371, val_loss: 1.01133, val_accuracy: 0.50803\n",
      "Epoch: 1, loss: 1.02004, accuracy: 0.50406, val_loss: 1.01002, val_accuracy: 0.50803\n",
      "Epoch: 2, loss: 1.01905, accuracy: 0.50406, val_loss: 1.01257, val_accuracy: 0.50803\n",
      "Epoch: 3, loss: 1.01836, accuracy: 0.50406, val_loss: 1.01345, val_accuracy: 0.50803\n",
      "Epoch: 4, loss: 1.01480, accuracy: 0.50397, val_loss: 0.99663, val_accuracy: 0.50803\n",
      "Epoch: 5, loss: 0.98966, accuracy: 0.51716, val_loss: 0.98157, val_accuracy: 0.53142\n",
      "Epoch: 6, loss: 0.97515, accuracy: 0.52667, val_loss: 0.96756, val_accuracy: 0.52793\n",
      "Epoch: 7, loss: 0.96881, accuracy: 0.53017, val_loss: 0.96502, val_accuracy: 0.53596\n",
      "Epoch: 8, loss: 0.96394, accuracy: 0.53174, val_loss: 0.96212, val_accuracy: 0.53736\n",
      "Epoch: 9, loss: 0.96381, accuracy: 0.53121, val_loss: 0.96498, val_accuracy: 0.53352\n",
      "Epoch: 10, loss: 0.96493, accuracy: 0.53217, val_loss: 0.97734, val_accuracy: 0.53492\n",
      "Epoch: 11, loss: 0.96278, accuracy: 0.53672, val_loss: 0.96858, val_accuracy: 0.53631\n",
      "Epoch: 12, loss: 0.96205, accuracy: 0.53436, val_loss: 0.95852, val_accuracy: 0.53841\n",
      "Epoch: 13, loss: 0.95987, accuracy: 0.53628, val_loss: 0.96086, val_accuracy: 0.53387\n",
      "Epoch: 14, loss: 0.95667, accuracy: 0.53811, val_loss: 0.95897, val_accuracy: 0.53876\n",
      "Epoch: 15, loss: 0.95545, accuracy: 0.53846, val_loss: 0.95705, val_accuracy: 0.54225\n",
      "Epoch: 16, loss: 0.94883, accuracy: 0.54003, val_loss: 0.96356, val_accuracy: 0.54434\n",
      "Epoch: 17, loss: 0.94443, accuracy: 0.54257, val_loss: 0.94482, val_accuracy: 0.54155\n",
      "Epoch: 18, loss: 0.94245, accuracy: 0.54126, val_loss: 0.95144, val_accuracy: 0.54120\n",
      "Epoch: 19, loss: 0.94000, accuracy: 0.54335, val_loss: 0.95388, val_accuracy: 0.53876\n",
      "Epoch: 20, loss: 0.93574, accuracy: 0.54518, val_loss: 0.96153, val_accuracy: 0.54120\n",
      "Epoch: 21, loss: 0.93417, accuracy: 0.55138, val_loss: 0.94964, val_accuracy: 0.53841\n",
      "Epoch: 22, loss: 0.93033, accuracy: 0.54571, val_loss: 0.94800, val_accuracy: 0.54015\n",
      "Epoch: 23, loss: 0.93059, accuracy: 0.55453, val_loss: 0.98084, val_accuracy: 0.50943\n",
      "Epoch: 24, loss: 0.93724, accuracy: 0.54990, val_loss: 0.94303, val_accuracy: 0.55098\n",
      "Epoch: 25, loss: 0.92964, accuracy: 0.55496, val_loss: 0.95594, val_accuracy: 0.54155\n",
      "Epoch: 26, loss: 0.93044, accuracy: 0.55435, val_loss: 0.95384, val_accuracy: 0.54365\n",
      "Epoch: 27, loss: 0.92853, accuracy: 0.55627, val_loss: 0.95313, val_accuracy: 0.53596\n",
      "Epoch: 28, loss: 0.92627, accuracy: 0.55531, val_loss: 0.94016, val_accuracy: 0.54155\n",
      "Epoch: 29, loss: 0.92489, accuracy: 0.55872, val_loss: 0.94548, val_accuracy: 0.53841\n",
      "Epoch: 30, loss: 0.92713, accuracy: 0.55269, val_loss: 0.95211, val_accuracy: 0.54190\n",
      "Epoch: 31, loss: 0.92760, accuracy: 0.55034, val_loss: 0.94391, val_accuracy: 0.54504\n",
      "Epoch: 32, loss: 0.92279, accuracy: 0.55191, val_loss: 0.94373, val_accuracy: 0.54295\n",
      "Epoch: 33, loss: 0.92229, accuracy: 0.55654, val_loss: 0.93985, val_accuracy: 0.53946\n",
      "Epoch: 34, loss: 0.92051, accuracy: 0.55715, val_loss: 0.94395, val_accuracy: 0.54015\n",
      "Epoch: 35, loss: 0.92284, accuracy: 0.55557, val_loss: 0.94336, val_accuracy: 0.53911\n",
      "Epoch: 36, loss: 0.91803, accuracy: 0.55601, val_loss: 0.93969, val_accuracy: 0.54155\n",
      "Epoch: 37, loss: 0.91783, accuracy: 0.55811, val_loss: 0.93446, val_accuracy: 0.54644\n",
      "Epoch: 38, loss: 0.91422, accuracy: 0.56064, val_loss: 0.93765, val_accuracy: 0.53666\n",
      "Epoch: 39, loss: 0.91596, accuracy: 0.56343, val_loss: 0.93760, val_accuracy: 0.54295\n",
      "Epoch: 40, loss: 0.90963, accuracy: 0.56448, val_loss: 0.94598, val_accuracy: 0.54155\n",
      "Epoch: 41, loss: 0.91204, accuracy: 0.55950, val_loss: 0.94382, val_accuracy: 0.54120\n",
      "Epoch: 42, loss: 0.90785, accuracy: 0.56439, val_loss: 0.94276, val_accuracy: 0.54190\n",
      "Epoch: 43, loss: 0.90661, accuracy: 0.56081, val_loss: 0.94322, val_accuracy: 0.54784\n",
      "Epoch: 44, loss: 0.90835, accuracy: 0.56335, val_loss: 0.95100, val_accuracy: 0.54504\n",
      "Epoch: 45, loss: 0.91087, accuracy: 0.56151, val_loss: 0.95509, val_accuracy: 0.53596\n",
      "Epoch: 46, loss: 0.91003, accuracy: 0.56151, val_loss: 0.95892, val_accuracy: 0.51606\n",
      "Epoch: 47, loss: 0.90697, accuracy: 0.56719, val_loss: 0.95581, val_accuracy: 0.53527\n",
      "Epoch: 48, loss: 0.90532, accuracy: 0.56308, val_loss: 0.96483, val_accuracy: 0.53911\n",
      "Epoch: 49, loss: 0.90136, accuracy: 0.56727, val_loss: 0.95284, val_accuracy: 0.54365\n",
      "Epoch: 50, loss: 0.90082, accuracy: 0.56562, val_loss: 0.95529, val_accuracy: 0.53911\n",
      "Epoch: 51, loss: 0.90446, accuracy: 0.56701, val_loss: 0.95472, val_accuracy: 0.54260\n",
      "Epoch: 52, loss: 0.90184, accuracy: 0.56727, val_loss: 0.97063, val_accuracy: 0.53108\n",
      "Epoch: 53, loss: 0.89888, accuracy: 0.56483, val_loss: 0.96608, val_accuracy: 0.53666\n",
      "Epoch: 54, loss: 0.89536, accuracy: 0.56946, val_loss: 0.95221, val_accuracy: 0.53980\n",
      "Epoch: 55, loss: 0.89523, accuracy: 0.56806, val_loss: 0.95933, val_accuracy: 0.54295\n",
      "Epoch: 56, loss: 0.89784, accuracy: 0.57112, val_loss: 0.95509, val_accuracy: 0.53980\n"
     ]
    }
   ],
   "source": [
    "history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0\n",
    "    val_loss_total = 0\n",
    "    correct_total = 0\n",
    "    val_correct_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        outputs = model.forward(samples[0]) #forward pass\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "        # obtain the loss function\n",
    "        loss = criterion(outputs, samples[1])\n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        correct_total += (samples[1].argmax(axis=1) == outputs.argmax(axis=1)).float().sum().item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx_test, samples in enumerate(test_dataloader):\n",
    "            outputs = model.forward(samples[0]) #forward pass\n",
    "            loss = criterion(outputs, samples[1])\n",
    "\n",
    "            val_loss_total += loss.item()\n",
    "            val_correct_total += (samples[1].argmax(axis=1) == outputs.argmax(axis=1)).float().sum().item() \n",
    "    \n",
    "    _loss = loss_total / (batch_idx+1)\n",
    "    _acc = correct_total / X_train.shape[0]\n",
    "    _val_loss = val_loss_total / (batch_idx_test+1)\n",
    "    _val_acc = val_correct_total / X_test.shape[0]\n",
    "    \n",
    "    history_logger(history, [_loss, _acc, _val_loss, _val_acc])\n",
    "    \n",
    "    print(\"Epoch: %d, loss: %1.5f, accuracy: %1.5f, val_loss: %1.5f, val_accuracy: %1.5f\" % \\\n",
    "          (epoch, _loss, _acc, _val_loss, _val_acc)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
