{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e73bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ade5a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11453, 6, 60) (11453, 3)\n",
      "(2864, 6, 60) (2864, 3)\n",
      "----\n",
      "torch.Size([11453, 6, 60]) torch.Size([11453, 3])\n",
      "torch.Size([2864, 6, 60]) torch.Size([2864, 3])\n"
     ]
    }
   ],
   "source": [
    "# Get the data and split into train, validation and test sets\n",
    "X = np.load('data/X.npy')\n",
    "X = np.moveaxis(X, 1, 2)\n",
    "y = np.load('data/y.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test)) \n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(X_train_tensors, y_train_tensors),\n",
    "                              batch_size=64, shuffle=True, num_workers=8) \n",
    "test_dataloader = DataLoader(TensorDataset(X_test_tensors, y_test_tensors),\n",
    "                             batch_size=64, shuffle=True, num_workers=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f175a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_test(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_test, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8d9bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = X_train.shape[2] #number of features\n",
    "hidden_size = 64 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 3 \n",
    "\n",
    "model = LSTM_test(num_classes, input_size, hidden_size, num_layers) \n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "517a3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_logger(history, data):\n",
    "    for idx, key in enumerate(history):\n",
    "        history[key].append(data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.02161, accuracy: 0.50371, val_loss: 1.01133, val_accuracy: 0.50803\n",
      "Epoch: 1, loss: 1.02004, accuracy: 0.50406, val_loss: 1.01002, val_accuracy: 0.50803\n",
      "Epoch: 2, loss: 1.01905, accuracy: 0.50406, val_loss: 1.01257, val_accuracy: 0.50803\n",
      "Epoch: 3, loss: 1.01836, accuracy: 0.50406, val_loss: 1.01345, val_accuracy: 0.50803\n",
      "Epoch: 4, loss: 1.01480, accuracy: 0.50397, val_loss: 0.99663, val_accuracy: 0.50803\n",
      "Epoch: 5, loss: 0.98966, accuracy: 0.51716, val_loss: 0.98157, val_accuracy: 0.53142\n",
      "Epoch: 6, loss: 0.97515, accuracy: 0.52667, val_loss: 0.96756, val_accuracy: 0.52793\n",
      "Epoch: 7, loss: 0.96881, accuracy: 0.53017, val_loss: 0.96502, val_accuracy: 0.53596\n",
      "Epoch: 8, loss: 0.96394, accuracy: 0.53174, val_loss: 0.96212, val_accuracy: 0.53736\n",
      "Epoch: 9, loss: 0.96381, accuracy: 0.53121, val_loss: 0.96498, val_accuracy: 0.53352\n",
      "Epoch: 10, loss: 0.96493, accuracy: 0.53217, val_loss: 0.97734, val_accuracy: 0.53492\n",
      "Epoch: 11, loss: 0.96278, accuracy: 0.53672, val_loss: 0.96858, val_accuracy: 0.53631\n",
      "Epoch: 12, loss: 0.96205, accuracy: 0.53436, val_loss: 0.95852, val_accuracy: 0.53841\n",
      "Epoch: 13, loss: 0.95987, accuracy: 0.53628, val_loss: 0.96086, val_accuracy: 0.53387\n",
      "Epoch: 14, loss: 0.95667, accuracy: 0.53811, val_loss: 0.95897, val_accuracy: 0.53876\n",
      "Epoch: 15, loss: 0.95545, accuracy: 0.53846, val_loss: 0.95705, val_accuracy: 0.54225\n",
      "Epoch: 16, loss: 0.94883, accuracy: 0.54003, val_loss: 0.96356, val_accuracy: 0.54434\n",
      "Epoch: 17, loss: 0.94443, accuracy: 0.54257, val_loss: 0.94482, val_accuracy: 0.54155\n",
      "Epoch: 18, loss: 0.94245, accuracy: 0.54126, val_loss: 0.95144, val_accuracy: 0.54120\n",
      "Epoch: 19, loss: 0.94000, accuracy: 0.54335, val_loss: 0.95388, val_accuracy: 0.53876\n",
      "Epoch: 20, loss: 0.93574, accuracy: 0.54518, val_loss: 0.96153, val_accuracy: 0.54120\n",
      "Epoch: 21, loss: 0.93417, accuracy: 0.55138, val_loss: 0.94964, val_accuracy: 0.53841\n",
      "Epoch: 22, loss: 0.93033, accuracy: 0.54571, val_loss: 0.94800, val_accuracy: 0.54015\n",
      "Epoch: 23, loss: 0.93059, accuracy: 0.55453, val_loss: 0.98084, val_accuracy: 0.50943\n",
      "Epoch: 24, loss: 0.93724, accuracy: 0.54990, val_loss: 0.94303, val_accuracy: 0.55098\n",
      "Epoch: 25, loss: 0.92964, accuracy: 0.55496, val_loss: 0.95594, val_accuracy: 0.54155\n",
      "Epoch: 26, loss: 0.93044, accuracy: 0.55435, val_loss: 0.95384, val_accuracy: 0.54365\n",
      "Epoch: 27, loss: 0.92853, accuracy: 0.55627, val_loss: 0.95313, val_accuracy: 0.53596\n",
      "Epoch: 28, loss: 0.92627, accuracy: 0.55531, val_loss: 0.94016, val_accuracy: 0.54155\n"
     ]
    }
   ],
   "source": [
    "history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0\n",
    "    val_loss_total = 0\n",
    "    correct_total = 0\n",
    "    val_correct_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        outputs = model.forward(samples[0]) #forward pass\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "        # obtain the loss function\n",
    "        loss = criterion(outputs, samples[1])\n",
    "        loss.backward() #calculates the loss of the loss function\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        correct_total += (samples[1].argmax(axis=1) == outputs.argmax(axis=1)).float().sum().item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx_test, samples in enumerate(test_dataloader):\n",
    "            outputs = model.forward(samples[0]) #forward pass\n",
    "            loss = criterion(outputs, samples[1])\n",
    "\n",
    "            val_loss_total += loss.item()\n",
    "            val_correct_total += (samples[1].argmax(axis=1) == outputs.argmax(axis=1)).float().sum().item() \n",
    "    \n",
    "    _loss = loss_total / (batch_idx+1)\n",
    "    _acc = correct_total / X_train.shape[0]\n",
    "    _val_loss = val_loss_total / (batch_idx_test+1)\n",
    "    _val_acc = val_correct_total / X_test.shape[0]\n",
    "    \n",
    "    history_logger(history, [_loss, _acc, _val_loss, _val_acc])\n",
    "    \n",
    "    print(\"Epoch: %d, loss: %1.5f, accuracy: %1.5f, val_loss: %1.5f, val_accuracy: %1.5f\" % \\\n",
    "          (epoch, _loss, _acc, _val_loss, _val_acc)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
